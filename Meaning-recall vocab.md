# ğŸ¤– Stewart et al. (2025)
**Can we reliably score meaning-recall vocabulary tests using AI?**  
*(Language Testing, in press)*

---

## ğŸ§­ Introduction

Traditional vocabulary tests often rely on **meaning-recognition formats** (e.g., multiple-choice), which are efficient but can inflate scores through guessing.  

**Meaning-recall tests**, by contrast, tap into real productive vocabulary knowledge but are time-consuming and require trained human ratersâ€”especially for multilingual responses.  

With advances in **Large Language Models (LLMs)** such as *GPT-4* and *Gemini*, AI-based scoring may now achieve human-like semantic judgment and reliability.

---

## â“ Research Questions

**RQ1.** Are LLM ratings stricter, more lenient, or statistically indistinguishable from human ratings?  
**RQ2.** Does scoring meaning-recall vocabulary tests with LLMs affect reliability compared to human scoring?  
**RQ3.** Are correlations between human and LLM ratings acceptable under inter-rater reliability standards?  
**RQ4.** When and why do LLM and human raters differ in their judgments?


## ğŸ“š Background
* **Recognition tests:** efficient but they may fail to fully reflect a learnerâ€™s true ability.
* **Recall tests:** more valid but labor-intensive to score.  
* **LLMs:** show strong *contextual and semantic matching* (Aryadoust 2023; Roever 2024).  
- Empirical evidence comparing **human vs. AI reliability** is still limited.  
- This study examines whether AI scoring can achieve **fairness, validity, and consistency** comparable to human assessment.

---

## âš™ Method (ë°©ë²•)
| êµ¬ë¶„ | ë‚´ìš© |
|------|------|
| **Participants** | ì¼ë³¸ EFL í•™ìŠµì 611ëª… (18â€“22ì„¸, ì˜ì–´ í•™ìŠµ 6ë…„ ì´ìƒ) |
| **Test Design** | Meaning recall vocabulary test (150 ë‹¨ì–´, BNC/COCA 1Kâ€“5K ìˆ˜ì¤€). ì°¸ê°€ìëŠ” ê° ì˜ì–´ ë‹¨ì–´ì˜ ì¼ë³¸ì–´ ì˜ë¯¸ ì‘ì„±. |
| **Raters** | ğŸ‘©â€ğŸ« Human 2 ëª… (ì˜ì¼ ì´ì¤‘ì–¸ì–´ ì±„ì ì)  ğŸ¤– AI 3 ì¢… (GPT-4o, Gemini 1.5, Llama 3-8B) |
| **Scoring Scheme** | Binary (1 = correct, 0 = incorrect). Prompt: â€œDoes this Japanese response convey the correct meaning of the English word?â€ |
| **Analysis** | ê¸°ìˆ í†µê³„ + Many-Facet Rasch Measurement (MFRM), Cronbachâ€™s Î±, Pearson r, ICC (ì±„ì ìê°„ ì‹ ë¢°ë„). |

---

## ğŸ“Š Results (ê²°ê³¼)

### ğŸ§© RQ1. Rating strictness and comparability
| ì£¼ìš” ê²°ê³¼ | ìš”ì•½ |
|------------|-------|
| **Mean score pattern** | Gemini (75.3) > Human1 (73.8) â‰ˆ GPT (71.5) â‰ˆ Human2 (70.2) > Llama (64.7) |
| **Interpretation** | GPT-4oì™€ GeminiëŠ” ì¸ê°„ ì±„ì ìì™€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŒ. <br> â†’ ë‘ ëª¨ë¸ ëª¨ë‘ **ì¸ê°„ ìˆ˜ì¤€ì˜ í‰ê·  ì ìˆ˜ ë° ì±„ì  ê²½í–¥**ì„ ë³´ì„. <br> LlamaëŠ” **ê°€ì¥ ë³´ìˆ˜ì ì¸(ì—„ê²©í•œ)** ì±„ì  ê²½í–¥ì„ ë‚˜íƒ€ëƒ„. |


### ğŸ§© RQ2. Reliability (ë‚´ì  ì¼ê´€ì„±)
| ì£¼ìš” ê²°ê³¼ | ìš”ì•½ |
|------------|-------|
| **Cronbachâ€™s Î±** | GPT (.961), Gemini (.960), Human (.962), Llama (.941) |
| **Interpretation** | GPTì™€ Geminiì˜ ì‹ ë¢°ë„ëŠ” **ì¸ê°„ ì±„ì ìì™€ ë™ì¼ ìˆ˜ì¤€(.96)** ìœ¼ë¡œ ì•ˆì •ì ì„. <br> LlamaëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì ìˆ˜ë¥¼ ë³´ì—¬ **ëª¨ë¸ í¬ê¸°(7B)ì˜ í•œê³„**ë¥¼ ë°˜ì˜í•¨. <br> â†’ **AI ì±„ì ì´ ì‹ ë¢°ë„ ì €í•˜ ì—†ì´ ì ìš© ê°€ëŠ¥**í•¨ì„ ì‹œì‚¬. |


### ğŸ§© RQ3. Humanâ€“AI Correlation and Inter-rater Agreement
| ì£¼ìš” ê²°ê³¼ | ìš”ì•½ |
|------------|-------|
| **Pearson r** | Humanâ€“AI ê°„ ìƒê´€ r > .95 |
| **ICC (Interclass Correlation)** | > .90 â†’ â€œExcellent agreementâ€ ìˆ˜ì¤€ |
| **Interpretation** | GPTì™€ Geminiì˜ ì ìˆ˜ëŠ” **ì¸ê°„ ê°„ ìƒê´€(.925)** ì„ ëŠ¥ê°€í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ìˆ˜ì¤€ìœ¼ë¡œ, **ì±„ì  ì¼ì¹˜ë„ê°€ ë§¤ìš° ë†’ìŒ.** <br> â†’ LLM ê¸°ë°˜ ì±„ì ì´ **ì¸ê°„ ì±„ì ì ê°„ ë³€ë™ ë²”ìœ„ ë‚´**ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•¨. |


### ğŸ§© RQ4. Disagreement and Error Analysis
| ì£¼ìš” ê²°ê³¼ | ìš”ì•½ |
|------------|-------|
| **Error patterns** | ì£¼ë¡œ ë‹¤ì˜ì–´Â·L1 ì „ì´ ì˜ë¯¸ (*develop, clinic, random, fabulous*) ë“±ì—ì„œ ë¶ˆì¼ì¹˜ ë°œìƒ |
| **Interpretation** | GPTÂ·Geminiì˜ ì˜¤ë¥˜ëŠ” **ì˜ë¯¸ í™•ì¥/ê´€ìš©ì  ìš©ë²• í•´ì„ ì°¨ì´**ë¡œ ì¸í•œ ê²½ë¯¸í•œ íŒë‹¨ ì°¨ì´ë¡œ, ì¸ê°„ ê°„ ì±„ì  ì°¨ì´ì™€ ìœ ì‚¬í•œ ì–‘ìƒ. <br> LlamaëŠ” **ëª…ë°±í•œ ì˜¤íŒ(ì˜ˆ: ì •ë‹µì„ ì˜¤ë‹µìœ¼ë¡œ)** ì‚¬ë¡€ ì¡´ì¬. <br> â†’ ì „ë°˜ì ìœ¼ë¡œ **AIì˜ ì˜¤ë¥˜ëŠ” í•´ì„ìƒì˜ ì°¨ì´ì— ë¶ˆê³¼í•˜ë©°, ì‹¬ê°í•œ ì˜¤íŒì€ ë“œë¬¾.** |

---

## ğŸŒˆ Overall Interpretation

* **GPT-4o** and **Gemini 1.5** show **human-equivalent reliability, correlation, and scoring consistency** in meaning-recall vocabulary tests.  
* **Llama 3-8B** â†’ parameters ì ì–´ ë³´ìˆ˜ì  ì±„ì  ê²½í–¥.  
* **AI scoring patterns â‰ˆ human judgment**, low-stakes í‰ê°€ ì— í™œìš© ê°€ëŠ¥.  
* ì°¨ì´ëŠ” ì£¼ë¡œ **polysemous words** ë° **L1 ì˜ë¯¸ ì „ì´** ìƒí™©ì—ì„œ ë°œìƒ. 

âœ… **Summary**
> LLMs (especially GPT-4o and Gemini 1.5) can **score meaning-recall vocabulary responses with human-level accuracy, reliability, and agreement**, marking a breakthrough in automated language assessment.

---

## ğŸ’¬ Discussion Questions
1. Should LLMs replace human raters in classroom vocabulary assessment?  
2. How can teachers ensure fairness for learners with different L1 backgrounds?  

